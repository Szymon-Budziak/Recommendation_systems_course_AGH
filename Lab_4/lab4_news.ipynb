{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorium 4 - rekomendacje dla portali informacyjnych\n",
    "\n",
    "## Przygotowanie\n",
    "\n",
    " * pobierz i wypakuj dataset: https://mind201910small.blob.core.windows.net/release/MINDsmall_train.zip\n",
    "   * więcej możesz poczytać tutaj: https://learn.microsoft.com/en-us/azure/open-datasets/dataset-microsoft-news\n",
    " * [opcjonalnie] Utwórz wirtualne środowisko\n",
    " `python3 -m venv ./recsyslab4`\n",
    " * zainstaluj potrzebne biblioteki:\n",
    " `pip install nltk sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Część 1. - przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-14T17:34:32.587256Z",
     "end_time": "2023-11-14T17:34:33.538238Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/szymonbudziak/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/szymonbudziak/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importujemy wszystkie potrzebne pakiety\n",
    "import codecs\n",
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# mozesz uzyc do obliczania najbardziej podobnych tekstow zamiast liczenia \"na piechote\"\n",
    "# ale pamietaj o dostosowaniu formatu danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-14T17:34:33.525081Z",
     "end_time": "2023-11-14T17:34:33.584132Z"
    }
   },
   "outputs": [],
   "source": [
    "# definiujemy potrzebne zmienne\n",
    "\n",
    "PATH = './MINDsmall_train'\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-14T17:34:33.525137Z",
     "end_time": "2023-11-14T17:34:33.837265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51282\n"
     ]
    }
   ],
   "source": [
    "# wczytujemy metadane artykułów\n",
    "\n",
    "def parse_news_entry(entry):\n",
    "    news_id, category, subcategory, title, abstract = entry.split('\\t')[:5]\n",
    "    return {\n",
    "        'news_id': news_id,\n",
    "        'category': category,\n",
    "        'subcategory': subcategory,\n",
    "        'title': title,\n",
    "        'abstract': abstract\n",
    "    }\n",
    "\n",
    "\n",
    "def get_news_metadata():\n",
    "    with codecs.open(f'{PATH}/news.tsv', 'r', 'UTF-8') as f:\n",
    "        raw = [x for x in f.read().split('\\n') if x]\n",
    "        parsed_entries = [parse_news_entry(entry) for entry in raw]\n",
    "        return {x['news_id']: x for x in parsed_entries}\n",
    "\n",
    "\n",
    "news = get_news_metadata()\n",
    "news_ids = sorted(list(news.keys()))\n",
    "news_indices = {x[1]: x[0] for x in enumerate(news_ids)}\n",
    "print(len(news))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Część 2. - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-14T17:34:33.864282Z",
     "end_time": "2023-11-14T17:34:54.838468Z"
    }
   },
   "outputs": [],
   "source": [
    "# normalizujemy teksty na potrzeby dalszego przetwarzania\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # zamieniamy wszystkie ciagi bialych znakow na pojedyncze spacje\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # usuwamy znaki interpunkcyjne\n",
    "    text = text.translate(str.maketrans('', '', punctuation))\n",
    "\n",
    "    # usuwamy wszystkie liczby\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "    # podmieniamy wszystkie wielkie litery\n",
    "    text = text.lower()\n",
    "\n",
    "    # dzielimy na tokeny\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # usuwamy stopwords (czyli najczęściej występujące słowa, które nie niosą dużo informacji)\n",
    "    return [word for word in tokens if word not in STOPWORDS]\n",
    "\n",
    "\n",
    "def stem_texts(corpus):\n",
    "    stemmer = SnowballStemmer(language='english')  # przetestuj rozne stemmery\n",
    "    return [[stemmer.stem(word) for word in preprocess_text(text)] for text in corpus]\n",
    "\n",
    "\n",
    "texts = [news[news_id]['abstract'] for news_id in news_ids]\n",
    "stemmed_texts = stem_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-14T17:34:54.844249Z",
     "end_time": "2023-11-14T17:34:54.846345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I think we have a really good team, and a team that can really do some special, good things because that group is very close in there.\" - Brian Schmetzer\n",
      "\n",
      "think realli good team team realli special good thing group close brian schmetzer\n"
     ]
    }
   ],
   "source": [
    "# porownajmy teksty przed i po przetworzeniu\n",
    "\n",
    "print(texts[2] + '\\n')\n",
    "print(' '.join(stemmed_texts[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-14T17:34:54.849706Z",
     "end_time": "2023-11-14T17:34:55.021931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41846\n"
     ]
    },
    {
     "data": {
      "text/plain": "['aa',\n 'aaa',\n 'aaaa',\n 'aaaaaaaand',\n 'aaaaaand',\n 'aaaaah',\n 'aac',\n 'aaf',\n 'aagdzga',\n 'aah']"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tworzymy liste wszystkich slow w korpusie\n",
    "\n",
    "def get_all_words_sorted(corpus):\n",
    "    # generujemy posortowana alfabetycznie liste wszystkich slow (tokenow)\n",
    "    all_words = []\n",
    "\n",
    "    # Przechodzimy przez każdy dokument w korpusie\n",
    "    for document in corpus:\n",
    "        # Dzielimy dokument na tokeny (słowa) i dodajemy do listy wszystkich słów\n",
    "        all_words.extend(document)\n",
    "\n",
    "    # Sortujemy listę alfabetycznie i ją zwracamy\n",
    "    return sorted(set(all_words))\n",
    "\n",
    "\n",
    "wordlist = get_all_words_sorted(stemmed_texts)\n",
    "word_indices = {x[1]: x[0] for x in enumerate(wordlist)}\n",
    "print(len(wordlist))\n",
    "wordlist[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-14T17:34:55.022111Z",
     "end_time": "2023-11-14T17:34:55.328055Z"
    }
   },
   "outputs": [],
   "source": [
    "# obliczamy liczbe tekstow, w ktorych wystapilo kazde ze slow\n",
    "# pamietaj, ze jesli slowo wystapilo w danym tekscie wielokrotnie, to liczymy je tylko raz\n",
    "\n",
    "def get_document_frequencies(corpus, wordlist):\n",
    "    # return {word -> count}\n",
    "    document_frequencies = {word: 0 for word in wordlist}\n",
    "\n",
    "    for text in corpus:\n",
    "        for word in set(text):\n",
    "            document_frequencies[word] += 1\n",
    "\n",
    "    return document_frequencies\n",
    "\n",
    "\n",
    "document_frequency = get_document_frequencies(stemmed_texts, wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-14T17:34:55.335571Z",
     "end_time": "2023-11-14T17:34:55.519005Z"
    }
   },
   "outputs": [],
   "source": [
    "# obliczamy liczbe wystapien kazdego slowa w kazdym tekscie\n",
    "# ile razy dane slowo w danym tekscie wystapilo\n",
    "\n",
    "def get_term_frequencies(corpus, news_indices):\n",
    "    # return {news_id -> {word -> count}}\n",
    "    return {news_id: Counter(text) for news_id, text in zip(news_indices, corpus)}\n",
    "\n",
    "\n",
    "term_frequency = get_term_frequencies(stemmed_texts, news_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-14T17:34:55.519256Z",
     "end_time": "2023-11-14T17:34:55.528852Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Counter({'realli': 2,\n         'good': 2,\n         'team': 2,\n         'think': 1,\n         'special': 1,\n         'thing': 1,\n         'group': 1,\n         'close': 1,\n         'brian': 1,\n         'schmetzer': 1})"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sprawdzmy wyniki\n",
    "\n",
    "term_frequency[news_ids[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-14T17:34:55.570568Z",
     "end_time": "2023-11-14T17:34:56.716404Z"
    }
   },
   "outputs": [],
   "source": [
    "# obliczamy metryke tf_idf\n",
    "\n",
    "def calculate_tf_idf(term_frequency, document_frequency, corpus_size):\n",
    "    # return {news_id -> {word -> tf_idf}}\n",
    "    tf_idf = {news_id: {word: 0 for word in term_frequency[news_id].keys()} for news_id in term_frequency.keys()}\n",
    "\n",
    "    for news_id in term_frequency.keys():\n",
    "        for word in term_frequency[news_id].keys():\n",
    "            tf_idf[news_id][word] = term_frequency[news_id][word] * math.log(corpus_size / document_frequency[word])\n",
    "\n",
    "    return tf_idf\n",
    "\n",
    "\n",
    "tf_idf = calculate_tf_idf(term_frequency, document_frequency, len(news_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-14T17:34:56.709937Z",
     "end_time": "2023-11-14T17:34:56.731602Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'think': 4.360459856758821,\n 'realli': 9.707261090572182,\n 'good': 8.085400658139525,\n 'team': 6.157356388748834,\n 'special': 4.765161897298483,\n 'thing': 4.111693200556713,\n 'group': 4.261685870235309,\n 'close': 3.9244235881453897,\n 'brian': 5.624739267315748,\n 'schmetzer': 9.458800731274183}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sprawdzmy wyniki\n",
    "\n",
    "tf_idf[news_ids[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Część 3. - Podobieństwo tekstów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-14T17:34:56.724627Z",
     "end_time": "2023-11-14T17:34:56.745218Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "42.495038054961356"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obliczmy odleglosc miedzy dwoma artykulami\n",
    "# przetestuj rozne metryki odleglosci i wybierz najlepsza\n",
    "\n",
    "def calculate_distance(tf_idf, id1, id2):\n",
    "    distance = 0\n",
    "\n",
    "    # metryka euklidesowa\n",
    "    for word in set(tf_idf[id1].keys()) | set(tf_idf[id2].keys()):\n",
    "        val1 = tf_idf[id1].get(word, 0)\n",
    "        val2 = tf_idf[id2].get(word, 0)\n",
    "        distance += (val1 - val2) ** 2\n",
    "    distance = math.sqrt(distance)\n",
    "\n",
    "    # metryka taksówkowa\n",
    "    # for word in set(tf_idf[id1].keys()) | set(tf_idf[id2].keys()):\n",
    "    #     val1 = tf_idf[id1].get(word, 0)\n",
    "    #     val2 = tf_idf[id2].get(word, 0)\n",
    "    #     distance += abs(val1 - val2)\n",
    "    # distance = math.sqrt(distance)\n",
    "\n",
    "    return distance\n",
    "\n",
    "\n",
    "calculate_distance(tf_idf, news_ids[2], news_ids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "'N58544'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_ids[42337]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-14T17:34:56.729530Z",
     "end_time": "2023-11-14T17:34:56.745570Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2023-11-14T17:34:56.777947Z",
     "end_time": "2023-11-14T17:34:59.662165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: N58544, text: A MAN claims he has created a car that might solve the world's traffic congestion problems. Rick Woodbury from Spokane, Washington USA, is the president, founder and sole employee of 'Commuter Cars.' The carmaker's flagship model is the 2005 super slim two-seater Tango T600, a high-performance electric car that preceded Tesla. Rick told BTV: \"I started this company 21 years ago   it was based on an idea that I came up with in 1982.\" He was inspired by the shocking traffic congestion he had to face on a daily basis. \"I used to drive a Porsche from Beverly Hills to Hermosa Beach every day and the traffic was horrendous,\" explained Rick. What really made Rick think about a solution was the fact that in most of the cars he would see in his commute were occupied by lone drivers. \"I noticed that everybody around me was a single occupant in a car, taking up the whole lane,\" Rick said. Living and working in Los Angeles also helped inspire Rick's unique creation. \"I thought, everyone wants to get from point A to point B efficiently, and in cities like Los Angeles there's really no centre, there's no hub, everybody goes everywhere,\" explained Rick. For him, there is a simple solution, and that is reducing the width that vehicles take up on the road so that 2 can fit comfortably in a single lane. \"I don't think there's any other answer except doubling lane capacity,\" said Rick. With a length of 102 inches this micro car can be parked just about anywhere, just like you would with a motorbike. Rick said: \"The length is the same width of a semi-truck so I can park perpendicular to the curb.\"The selling point of this vehicle is that it can drive in-between cars better than any other car. \"The coolest feature for me to that it can get through traffic faster than any car in history,\" said Rick. Much like a motorbike, the Tango T500 can drive right up to the stop line of traffic lights.\n",
      "\n",
      "5 most similar:\n",
      "\n",
      "id: N51828, text: President Trump to make history as first president to attend parade; Rick Leventhal reports from Manhattan.\n",
      "\n",
      "id: N31628, text: Former Texas Governor Rick Perry is the first member of President Donald Trump's Cabinet asked to appear before the House.\n",
      "\n",
      "id: N61884, text: Former Texas Governor Rick Perry is the first member of President Donald Trump's Cabinet asked to appear before the House.\n",
      "\n",
      "id: N13422, text: Energy Secretary Rick Perry said the controversy over the Trump administration's handling of Ukraine had nothing to do with his resignation.\n",
      "\n",
      "id: N31126, text: Rick Perry and several top Trump administration aides are scheduled to testify in House Democrats' impeachment inquiry\n"
     ]
    }
   ],
   "source": [
    "# wyznaczmy k najpodobniejszych tekstow do danego\n",
    "# pamietaj o odpowiedniej kolejnosci sortowania w zaleznosci od wykorzystanej metryki\n",
    "# pamietaj, zeby wsrod podobnych tekstow nie bylo danego\n",
    "\n",
    "def get_k_most_similar_news(tf_idf, n_id, k):\n",
    "    distances = {}\n",
    "\n",
    "    for news_id in tf_idf.keys():\n",
    "        if news_id != n_id:\n",
    "            distances[news_id] = calculate_distance(tf_idf, news_id, n_id)\n",
    "\n",
    "    return dict(sorted(distances.items(), key=lambda x: x[1])[:k]).keys()\n",
    "\n",
    "\n",
    "def print_k_most_similar_news(tf_idf, n_id, k, corpus, news_indices):\n",
    "    similar = get_k_most_similar_news(tf_idf, n_id, k)\n",
    "    print(f'id: {n_id}, text: {corpus[news_indices[n_id]]}')\n",
    "    print(f'\\n{k} most similar:')\n",
    "    for s_id in similar:\n",
    "        print(f'\\nid: {s_id}, text: {corpus[news_indices[s_id]]}')\n",
    "\n",
    "\n",
    "print_k_most_similar_news(tf_idf, news_ids[42337], 5, texts, news_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
